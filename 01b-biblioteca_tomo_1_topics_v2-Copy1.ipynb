{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tomo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from gensim import corpora, models\n",
    "from nltk.corpus import stopwords\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def key_sort_files(x):\n",
    "    return int(x[:-4].split('-')[0])\n",
    "\n",
    "path = 'data/aux/biblioteca/text_parts/1/'\n",
    "unsorted_file_list = [filename for filename in os.listdir(path) if filename.endswith('.txt')]\n",
    "file_list = sorted(unsorted_file_list, key=key_sort_files)\n",
    "\n",
    "raw_texts = []\n",
    "for filename in file_list:\n",
    "    with open(path + filename) as f:\n",
    "        raw_texts.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = []\n",
    "for text in raw_texts:\n",
    "    for c in text:\n",
    "        if not c.isalnum():\n",
    "            chars.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "characters = set(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/stopwords/spanish_stopwords.txt') as f:\n",
    "    sp_stopwords = list(set(map(str.strip, f.readlines())))\n",
    "\n",
    "with open('data/stopwords/my_stopwords.txt') as f:\n",
    "    my_stopwords = list(set(map(str.strip, f.readlines())))\n",
    "\n",
    "stop = stopwords.words('spanish') + sp_stopwords + my_stopwords + list(punctuation) + list(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    r = s.lower().strip()\n",
    "    for c in characters:\n",
    "        r = r.replace(c, ' ')\n",
    "    r = r.replace('farc ep', 'farc-ep')\n",
    "    r = r.replace('confianz a', 'confianza')\n",
    "    r = r.replace('cons trucción', 'construcción')\n",
    "    rs = [w for w in nltk.word_tokenize(r) if w not in stop and len(w) > 2 and not w.isdecimal()]\n",
    "    r = ' '.join(rs)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_texts = [clean(text) for text in raw_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {' ': 37776, '-': 417})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_counter = defaultdict(int)\n",
    "for c in characters:\n",
    "    for text in cleaned_texts:\n",
    "        if c in text:\n",
    "            char_counter[c]+=text.count(c)\n",
    "char_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document = ' '.join(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only texts that contain MORE THAN or EQUALS TO 100 words\n",
    "texts = [[word for word in document.split()] for document in documents if len(document.split()) >= 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8832"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_vocabulary(ts):\n",
    "    vocab = set()\n",
    "    for t in ts:\n",
    "        words = re.findall('\"(\\w+)\"', t)\n",
    "        vocab = vocab.union(words)\n",
    "    vocab = sorted(vocab)\n",
    "    r = {w: i for i, w in enumerate(vocab)}\n",
    "    return r\n",
    "\n",
    "def get_weights(t):\n",
    "    weights = re.findall('(0.\\d{3})\\*\"(\\w+)\"', t)\n",
    "    r = {wo: float(we) for we, wo in weights}\n",
    "    return r\n",
    "\n",
    "def create_vector(vocab, weights):\n",
    "    r = [0] * len(vocab)\n",
    "    for wo in weights:\n",
    "        r[vocab[wo]] = weights[wo]\n",
    "    return r\n",
    "\n",
    "def vectorize(ts):\n",
    "    r = []\n",
    "    ts = [t for i, t in ts]\n",
    "    vocab = extract_vocabulary(ts)\n",
    "    for t in ts:\n",
    "        weights = get_weights(t)\n",
    "        vector = create_vector(vocab, weights)\n",
    "        r.append(vector)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def two_topics_distance(t1, t2):\n",
    "    r = spatial.distance.cosine(t1, t2)\n",
    "    return r\n",
    "\n",
    "def multiple_topics_distance(ts):\n",
    "    ds = []\n",
    "    for i, t1 in enumerate(ts[:-1]):\n",
    "        for j in range(i + 1, len(ts)):\n",
    "            t2 = ts[j]\n",
    "            d = two_topics_distance(t1, t2)\n",
    "            ds.append(d)\n",
    "    return np.mean(ds)\n",
    "\n",
    "def best_topics(ts_lst):\n",
    "    r = []\n",
    "    for ts in ts_lst:\n",
    "        ts_vector = vectorize(ts)\n",
    "        d = multiple_topics_distance(ts_vector)\n",
    "        r.append((d, ts))\n",
    "    r = sorted(r, reverse=True)\n",
    "    return r[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts1=[(0,\n",
    "  '0.006*\"casa\" + 0.005*\"cocina\" + 0.002*\"coche\"'),\n",
    " (1,\n",
    "  '0.007*\"coche\" + 0.006*\"motor\" + 0.003*\"carretera\"'),\n",
    " (2,\n",
    "  '0.007*\"colegio\" + 0.004*\"casa\" + 0.003*\"coche\"'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0.006, 0.002, 0.005, 0, 0],\n",
       " [0.003, 0, 0.007, 0, 0, 0.006],\n",
       " [0, 0.004, 0.003, 0, 0.007, 0]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize(ts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts2=[(0,\n",
    "  '0.009*\"comedor\" + 0.003*\"cocina\" + 0.002*\"casa\"'),\n",
    " (1,\n",
    "  '0.007*\"coche\" + 0.006*\"motor\" + 0.003*\"carretera\"'),\n",
    " (2,\n",
    "  '0.007*\"colegio\" + 0.004*\"aula\" + 0.003*\"casa\"'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts_lst = [ts1, ts2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.009*\"comedor\" + 0.003*\"cocina\" + 0.002*\"casa\"'),\n",
       " (1, '0.007*\"coche\" + 0.006*\"motor\" + 0.003*\"carretera\"'),\n",
       " (2, '0.007*\"colegio\" + 0.004*\"aula\" + 0.003*\"casa\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_topics(ts_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_df = 5\n",
    "max_df = 0.6\n",
    "max_features = 8832\n",
    "dictionary.filter_extremes(no_below=min_df, no_above=max_df, keep_n=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(984 unique tokens: ['firmado', 'lucha', 'ampliar', 'subsidios', 'dinámica']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=35, num_nnz=8357)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_TOPICS = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=N_TOPICS) # initialize an LSI transformation\n",
    "corpus_lsi = lsi[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lsi.print_topics(lsi.num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 15),\n",
       " (1, 3),\n",
       " (2, 3),\n",
       " (3, 2),\n",
       " (12, 2),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (11, 1),\n",
       " (14, 1),\n",
       " (15, 1),\n",
       " (16, 1),\n",
       " (23, 1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_predominant_topic(doc):\n",
    "    return sorted(doc, key=lambda x: abs(x[1]), reverse=True)[0][0]\n",
    "\n",
    "predominant_topics = []\n",
    "for doc in corpus_lsi: # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "    predominant_topics.append(get_predominant_topic(doc))\n",
    "Counter(predominant_topics).most_common()\n",
    "# result: (topic_id, number of documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_TOPICS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus, id2word=dictionary, num_topics=N_TOPICS)\n",
    "corpus_lda = lda[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.015*\"farc\" + 0.008*\"zona\" + 0.007*\"general\" + 0.006*\"alto\" + 0.006*\"conversaciones\" + 0.006*\"carlos\" + 0.006*\"mesa\" + 0.006*\"marco\" + 0.006*\"seguridad\" + 0.005*\"comisionado\"'),\n",
       " (1,\n",
       "  '0.010*\"mesa\" + 0.008*\"participación\" + 0.007*\"agenda\" + 0.007*\"desarrollo\" + 0.006*\"marco\" + 0.006*\"exploratorio\" + 0.006*\"general\" + 0.005*\"fase\" + 0.005*\"negociación\" + 0.005*\"guerra\"'),\n",
       " (2,\n",
       "  '0.012*\"farc\" + 0.009*\"exploratorio\" + 0.009*\"delegados\" + 0.007*\"negociación\" + 0.006*\"víctimas\" + 0.006*\"armado\" + 0.006*\"noruega\" + 0.006*\"agenda\" + 0.005*\"primera\" + 0.005*\"marco\"'),\n",
       " (3,\n",
       "  '0.007*\"agenda\" + 0.007*\"farc\" + 0.007*\"mesa\" + 0.007*\"desarrollo\" + 0.006*\"participación\" + 0.006*\"años\" + 0.006*\"víctimas\" + 0.005*\"delegados\" + 0.005*\"colombianos\" + 0.005*\"seguridad\"'),\n",
       " (4,\n",
       "  '0.014*\"víctimas\" + 0.008*\"farc\" + 0.007*\"mesa\" + 0.007*\"general\" + 0.006*\"hoy\" + 0.006*\"colombianos\" + 0.005*\"desarrollo\" + 0.005*\"años\" + 0.005*\"negociación\" + 0.005*\"partes\"'),\n",
       " (5,\n",
       "  '0.011*\"delegados\" + 0.007*\"venezuela\" + 0.007*\"partes\" + 0.007*\"farc\" + 0.006*\"desarrollo\" + 0.006*\"social\" + 0.006*\"negociación\" + 0.006*\"noruega\" + 0.005*\"víctimas\" + 0.005*\"exploratorio\"')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lda.print_topics(N_TOPICS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 10), (3, 8), (0, 5), (2, 4), (4, 4), (5, 4)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_predominant_topic(doc):\n",
    "    return sorted(doc, key=lambda x: abs(x[1]), reverse=True)[0][0]\n",
    "\n",
    "predominant_topics = []\n",
    "for doc in corpus_lda: # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "    predominant_topics.append(get_predominant_topic(doc))\n",
    "Counter(predominant_topics).most_common()\n",
    "# result: (topic_id, number of documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N_TOPICS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdp = models.HdpModel(corpus, id2word=dictionary)\n",
    "corpus_hdp = hdp[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(hdp.print_topics(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 5),\n",
       " (0, 3),\n",
       " (1, 3),\n",
       " (2, 2),\n",
       " (7, 2),\n",
       " (9, 2),\n",
       " (21, 2),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (8, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (15, 1),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 1),\n",
       " (27, 1),\n",
       " (29, 1)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_predominant_topic(doc):\n",
    "    return sorted(doc, key=lambda x: abs(x[1]), reverse=True)[0][0]\n",
    "\n",
    "predominant_topics = []\n",
    "for doc in corpus_hdp: # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "    predominant_topics.append(get_predominant_topic(doc))\n",
    "Counter(predominant_topics).most_common()\n",
    "# result: (topic_id, number of documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
